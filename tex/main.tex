\documentclass[12pt]{report}
\makeatletter
% Allow compiling from repo root or tex/ by expanding the input search path.
\def\input@path{{./}{./tex/}}
\makeatother
\usepackage{url}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[normalem]{ulem}
\usepackage{tabularray}
\usepackage{adjustbox}
\usepackage{lipsum}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{cleveref}
\usepackage{textcomp}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\let\CheckCommand\providecommand
\usepackage{microtype}
\usepackage{stmaryrd}
\usepackage{float}
\usepackage{pdfpages}
\usepackage{diagbox}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usetikzlibrary{arrows, calc, positioning, tikzmark, automata, arrows.meta, decorations.markings}
\pgfplotsset{compat=1.18}
\usepackage{titlesec}

% Numbers
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}

% Fraktur
\newcommand{\A}{\ensuremath{\mathfrak A}}
\newcommand{\B}{\ensuremath{\mathfrak B}}
\newcommand{\I}{\ensuremath{\mathfrak I}}

% Logic
\newcommand{\xor}{\ensuremath{\oplus}}
\newcommand{\impl}{\ensuremath{\rightarrow}}

% Enumerations
\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\roman{enumii})}
\usepackage{enumitem}

\usepackage{bussproofs}
\newcommand{\yields}{\ensuremath{\mathrel{\Rightarrow}}}

\tikzset{
    graph node/.style = {draw, thick},
    labelled node/.style = {graph node, execute at begin node = {\strut}, execute at end node = {\strut}},
    round node/.style = {labelled node, shape = circle, inner sep = 1pt, minimum size = 9mm},
    rectangular node/.style = {labelled node, shape = rectangle, inner xsep = 5pt, inner ysep = 1pt, minimum size = 8mm},
    rounded node/.style = {labelled node, shape = rectangle, rounded corners = 4mm, inner ysep = 1pt, inner xsep = 5pt, minimum size = 9mm},
    player 0 node/.style = {round node, font = {\Large}},
    player 1 node/.style = {rectangular node, font = {\Large}},
    verifier node/.style = {rounded node, font = {\large}},
    falsifier node/.style = {rectangular node, font = {\large}},
    graph edge/.style = {draw, thick, shorten < = 2pt, shorten > = 2pt, > = {Latex[round, length = 2.5mm, width = 2.5mm]}},
    highlighted edge/.style = {graph edge, blue}
}

\lstset{
  language=Python,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue}\bfseries,
  stringstyle=\color{red},
  commentstyle=\color{gray},
  showstringspaces=false,
  numbers=none,
  numberstyle=\tiny\color{gray},
  frame=single,
  breaklines=true,
}

\setmarginsrb{3 cm}{2.5 cm}{3 cm}{2.5 cm}{1 cm}{1.5 cm}{1 cm}{1.5 cm}

\title{Design and Implementation of a Data Warehouse for Product Engagement Analysis: A Case Study of Notion}
\author{Nick Zerjeski}
\date{November 2025}

\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

\pagestyle{fancy}
\fancyhf{}
\rhead{\theauthor}
\lhead{Data Warehouse for Product Engagement Analysis in Notion}
\cfoot{\thepage}

\begin{document}

% ------------------------------ TITLE PAGE ------------------------------
\begin{titlepage}
  \centering
  \vspace*{0.5 cm}
  
  \rule{\linewidth}{0.2 mm} \\[0.4 cm]
  {\huge \bfseries \thetitle}\\
  \rule{\linewidth}{0.2 mm} \\[1.5 cm]


  \begin{flushright}
    \large\theauthor\\
    \large\thedate
  \end{flushright}
\end{titlepage}

\tableofcontents
\newpage
\chapter{Domain Analysis and Description}\label{ch:1}
% Select a company and describe its domain
As the title suggests, in this case study we look at Notion. Notion is a Software-as-a-Service (SaaS) company that provides an all-in-one productivity and collaboration platform. It enables individuals and teams to create, organize, and share knowledge through a unified interface that combines note-taking, database management, task tracking, and wiki functionality. The platform is structured around \textit{workspaces} (multi-tenant units), which contain \textit{pages} composed of modular \textit{blocks} such as text, images, tables, and databases. These blocks are the atomic units of content and interaction. Workspaces can have multiple users, permission hierarchies, and integrations with external tools via API's.

% Describe the data landscape
The Notion ecosystem generates many voluminous data streams across several domains such as data about the Content, the User, Billing Data, Marketing and the Product Usage. In this case study we will focus on the Product Usage and Engagement. That is Event-level logs that captures user actions such as page creation, edits, views, comments, shares and API interactions. This data allows for temporal and behavioral analytics at high granularity.

% Data Warehouse Motivation (in this context)
A data warehouse is essential for Notion because its operational data is fragmented across multiple systems which are not optimized for analysis. Since the data is scattered it must be integrated into a consistent, subject-oriented repository to enable meaningful insights. A data warehouse provides stable, time-variant storage that preserves historical information necessary for analyzing trends in engagement and feature usage. This allows for fast, flexible querying of complex behavioral data. It also improves the data quality and consistency by cleaning and using heterogeneous sources. Therefore, a Data Warehouse enables us to transform raw event data into actionable intelligence. Ultimately, it provides the foundation for evidence-based decision-making and continuous product optimization.

\section{Key Analytical Questions}\label{sec:1.1}
The following five questions should later able to be answered by running queries on or data warehouse.

\begin{itemize}
\item \textbf{How does user engagement evolve over time across subscription tiers?}
This question reveals differences in activity between free and paid plans, supporting decisions on pricing and feature differentiation.

\item \textbf{Which content types (pages, databases, wikis, task boards) generate the highest sustained interaction rates?}  
Identifying the most engaging content structures helps prioritize feature development and template curation.

\item \textbf{What are the activation rates of new users within their first seven days?}  
Activation is a leading indicator of retention; understanding early engagement patterns enables targeted onboarding improvements.

\item \textbf{How does device usage (web, mobile, desktop) affect user activity and session duration?}  
Device-level analysis supports product optimization and informs cross-platform development priorities.

\item \textbf{What proportion of total activity originates from collaborative versus individual work?}  
Measuring the balance between solo and shared content usage reveals how effectively Notion fosters team adoption within workspaces.
\end{itemize}

\section{Key Performance Indicators (KPIs)}
Each analytical question corresponds to one or more measurable KPIs central to product health evaluation:

\begin{itemize}
\item \textbf{Daily/Weekly/Monthly Active Users (DAU/WAU/MAU):} Core activity metrics measuring recurring usage patterns.
\item \textbf{Stickiness Ratio (DAU/MAU):} Indicates habitual engagement and long-term adoption strength.
\item \textbf{Activation Rate:} Percentage of new users performing a predefined minimum number of meaningful interactions (e.g., seven content edits within seven days).
\item \textbf{Feature Adoption Rate:} Share of users engaging with advanced functionalities such as databases, templates, or integrations.
\item \textbf{Average Session Depth:} Mean number of interactions per user session, used to evaluate content richness and user intent.
\end{itemize}

\section{Granularity Discussion}
Since we are trying to analyze \textbf{Product Usage and Engagement} the appropriate granularity in this context are \textbf{Event-level facts}. Event-level facts capture each individual user action that occurs within a workspace, including page creations, edits, views, comments, shares, and API-driven interactions tied to a timestamp. Therefore, this level of detail is fine enough to compute all engagement-related KPIs. Furthermore, it ensures that every analytical question can be answered, since each fact retains its full context regarding each dimension. Using this level of granularity ensures analytical flexibility and avoids premature aggregation that would cause problems later one if a finer granularity would be required.


\chapter{Conceptual Design}
To build the Dimensional Fact Model, we first think about which dimensions and measures our fact should have. Since we are describing the Product Usage and Engagement our fact will represent a single atomic user event in Notion such as these described in Chapter \ref{ch:1}. To capture the full behavioral context, we need to capture also the user, the content object, the workspace, the device, the event type, the session and the timestamp, which we do via dimensions. We therefore have the following dimensions:
\input{tables/dim-user}

\input{tables/dim-content}

\input{tables/dim-workspace}

\input{tables/dim-device}

\input{tables/dim-event}

\input{tables/dim-session}

\input{tables/dim-time}
To evaluate product usage and engagement effectively, the fact table must also include measures that quantify how users interact with Notion. Therefore, the following measures allow us to aggregate individual events into meaningful indicators:

\textbf{Event Count}
\begin{itemize} % Important for possible aggregations later on
    \item \textbf{Description:} Always 1 per event.
    \item \textbf{Additivity:} Fully additive across all dimensions.
\end{itemize}

\textbf{Active User Flag}
\begin{itemize}
    \item \textbf{Description:} 1 on a user's first event of a day, 0 otherwise.
    \item \textbf{Additivity:} Additive across users for a fixed day; semi-additive across time.
\end{itemize}

\textbf{Activation Event Flag}
\begin{itemize}
    \item \textbf{Description:} 1 if the event is a meaningful interaction within the first seven days after signup.
    \item \textbf{Additivity:} Additive within user-time windows; non-additive when converted into activation rates.
\end{itemize}

\textbf{Feature Usage Flag}
\begin{itemize}
    \item \textbf{Description:} 1 if the event uses an advanced feature.
    \item \textbf{Additivity:} Additive for counts; non-additive when used in ratios.
\end{itemize}

\textbf{Collaboration Event Flag}
\begin{itemize}
    \item \textbf{Description:} 1 for events occurring in a collaborative context.
    \item \textbf{Additivity:} Additive for counts; non-additive for proportions.
\end{itemize}

\textbf{Session Event Count}
\begin{itemize}
    \item \textbf{Description:} Number of events per session (sum of Event Count grouped by session).
    \item \textbf{Additivity:} Additive within a session; semi-additive across time.
\end{itemize}

\textbf{Session Duration Seconds}
\begin{itemize}
    \item \textbf{Description:} Stored on the final event of a session.
    \item \textbf{Additivity:} Additive across sessions; semi-additive across time.
\end{itemize}


\section{Dimensional Fact Model}
Since we now discussed and defined the dimensions and measures of our Product Usage and Engagement fact, we can start building the Dimensional Fact Model (DFM).

\input{figures/dfm}
TODO: Explaining the DFM and why I choose some of the design choices I made

\chapter{Logical Design}\label{ch:3}
Since all hierarchies are shallow and local to their dimensions and no hierarchy is reused across multiple dimensions, there is no need to normalize those hierarchies into additional tables. To keep the number of joins to a minimum and the queries as simple as possible, especially with no significant gain in storage efficiency, we are using a star schema for our \textit{Product Usage and Engagement} fact. Therefore, we have the following fact table, followed by the dimension tables.

\input{tables/fact-table}

\input{tables/time-user-tables}
\input{tables/workspace-content-tables}
\input{tables/device-event-tables}
\input{tables/session-table}
To get an overview on how much space we will be using, we estimate the cardinality of our tables. For that, we assume that we save our data for two years. Furthermore, we assume that we have a user base of around five million users from which roughly \(20\%\) are active on a given day and produce around \(10\) interactions. Therefore, we have the following size of our fact table:
\[5\text{Mio User }\cdot 20\%\cdot10\text{ Interactions }\cdot730\text{ days}=7.3\cdot 10^9\text{ rows}\]
We further assume, that each workspace has on average ten users, which yields a total of \(500\)k rows for the workspace table. If each of the workspaces has around 50 content objects in it, the content table has \(25\)Mio rows. For the device dimension, we simply assume that there are \(100\) different devices available and we also have around 100 different types of events. Finally, for the session, we assume a day is a session. Therefore, we have 
\[5\text{Mio User }\cdot 20\%\cdot730\text{ days}=7.3\cdot 10^8\text{ rows}\]
for the session table. To simplify the calculation of the size of the table, we assume that each column takes \(8\) Bytes. With that our tables have the sizes specified in Table \ref{tab:siz}. Keep in mind that some values are rounded.
\input{tables/table-sizes}


\section{Demonstration}
Now that we have the logical model in place, it is ready to support analytical workflows. To demonstrate how our schema answers to the defined business questions in section \ref{sec:1.1}, we select two of them and express them directly in SQL to extract the data from our  newly created fact and dimension tables. To be able to do this, we first have to populate our tables with some mock data. Note that only attributes are specified, that are needed for the analytical query.

\input{tables/mock-fact-usage}

\input{tables/mock-time-user}

\input{tables/mock-content}

With the mock data in place, we can go to our business questions. As first question we choose
\begin{quote}
    How does user engagement evolve over time across subscription tiers?
\end{quote}
This is an important question for subscription-based SaaS products, since different user tiers often exhibit different engagement patterns. Understanding how frequently users in each tier interact with the platform over time allows to identify trends in adoption, retention, and value perception. In Listing \ref{lst:bq1} you can see the query that extracts the information needed for answering this question.
\input{listings/bq1-query}
The result of running this query can be seen in Table \ref{tbl:bq1-results}.
\input{tables/bq1-results}

The second business question for which we run the query on is
\begin{quote}
    Which content types generate the highest interaction rates?
\end{quote}
This is an important business question because different content types are related to different workflows, and their usage patterns are strong indicators of feature adoption. Analyzing it provides insights on how future features can be developed. In Listing \ref{lst:bq2} you can see the query that extracts the information needed for answering this question.
\input{listings/bq2-query}
The result of this query can be seen in Table \ref{tbl:bq2-results}.
\input{tables/bq2-results}
\chapter{Physical Design}
Now that the logical design is in place, we can start to implement it in a real database system. For that, we create a script that creates all the tables with their corresponding surrogate keys and relations we defined in Chapter \ref{ch:3}. A snippet of this script can be seen in Listing \ref{lst:cs1}. Keep in mind that the full scripts are provided as appendix.
\input{listings/create-snippet}
Here we can see that every attribute with a \_key postfix is a surrogate key. As you can see in Table \ref{tab:siz}, the fact table can get rather big. It is therefore useful to split it into smaller tables. For our Key Analytical Questions we want to answer later on, it is beneficial to partition the table on the time dimension.

To populate our data warehouse, we first have to define in which time span we want to have data in it. Since data warehouses normally store data for up to two years, we choose January 2024 to December 2025 as the time span in which we want to store data in it. Of course, we can not simply add random data to it, because then we couldn't get meaningful data to answer our analytical questions. Therefore, we did the following assumptions: 


First, we created monthly partitions for the fact table so that the data fits into the chosen time span and queries stay fast. Then we filled the time dimension with one row per day from 2024-01-01 to 2025-12-31 so that every event can be linked to a calendar day. We also set up small, fixed device, content, and event dimensions with realistic options - for example desktop vs. mobile, wiki vs. or database vs. page - to make sure we have meaningful categories instead of random strings.

For the user dimension, we generated 20 new users per month across the whole time span, which yields 480 total users. We varied subscription tier, user type, region, and life cycle stage in a deterministic way so each month is stable in that regard and we can compare Free vs. Plus vs. Business vs. Enterprise behavior. Workspaces were created separately with at least 50 entries that mix plans, size buckets, industries, and regions to keep workspaces varied without being chaotic.

Sessions were added at about five per day, with durations between roughly 20 and 70 minutes. This gives enough session coverage so that every event can find a plausible session on the same day.

Finally, the fact table generation ties everything together. We compute an activation probability per user based on their tier and only give activation events to users who pass that threshold in their first week. On top of that, we assign monthly event volumes per user that scale with their tier, then sprinkled some randomness in it to avoid overly uniform behavior.

For each event we pick a date within the valid window, that is an event can only occur after a user created their account, attach biased mixes for content types, devices, and event types, and link the event to a workspace and a same-day session. The measure flags are set according to the type of event and content so that activation, feature usage, and collaboration metrics come out believable. This way the warehouse ends up with structured, meaningful mock data so we are ready to answer our key analytical questions. All scripts that answers those questions are provided in the appendix.

For our first question, we wanted to know how the user engagement evolves over multiple tiers.

\input{figures/kaq1}

\end{document}

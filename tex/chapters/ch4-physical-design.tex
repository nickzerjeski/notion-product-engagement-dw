\chapter{Physical Design}\label{ch:4}
Now that the logical design is in place, we can start to implement it in a real database system. For that, we create a script that creates all the tables with their corresponding surrogate keys and relations we defined in Chapter \ref{ch:3}. A snippet of this script can be seen in Listing \ref{lst:cs1}. Keep in mind that the full scripts are provided as appendix.
\input{../listings/create-snippet}
Here we can see that every attribute with a \_key postfix is a surrogate key. As you can see in Table \ref{tab:siz}, the fact table can get rather big. It is therefore useful to split it into smaller tables. For our Key Analytical Questions we want to answer later on, it is beneficial to partition the table on the time dimension.

To populate our data warehouse, we first have to define in which time span we want to have data in it. Since data warehouses normally store data for up to two years, we choose January 2024 to December 2025 as the time span in which we want to store data in it. Of course, we can not simply add random data to it, because then we couldn't get meaningful data to answer our analytical questions. Therefore, we did the following assumptions: 


First, we created monthly partitions for the fact table so that the data fits into the chosen time span and queries stay fast. Then we filled the time dimension with one row per day from 01.01.2024 to 31.12.2025 so that every event can be linked to a calendar day. We also set up small, fixed device, content, and event dimensions with realistic options - for example desktop vs. mobile, wiki vs. or database vs. page - to make sure we have meaningful categories instead of random strings.

For the user dimension, we generated 20 new users per month across the whole time span, which yields 480 total users. We varied subscription tier, user type, region, and life cycle stage in a deterministic way so each month is stable in that regard and we can compare Free vs. Plus vs. Business vs. Enterprise behavior. Workspaces were created separately with at least 50 entries that mix plans, size buckets, industries, and regions to keep workspaces varied without being chaotic.

Sessions were added at about five per day, with durations between roughly 20 and 70 minutes. This gives enough session coverage so that every event can find a plausible session on the same day.

Finally, the fact table generation ties everything together. We compute an activation probability per user based on their tier and only give activation events to users who pass that threshold in their first week. On top of that, we assign monthly event volumes per user that scale with their tier, then sprinkled some randomness in it to avoid overly uniform behavior.

For each event we pick a date within the valid window, that is an event can only occur after a user created their account, attach biased mixes for content types, devices, and event types, and link the event to a workspace and a same-day session. The measure flags are set according to the type of event and content so that activation, feature usage, and collaboration metrics come out believable. This way the warehouse ends up with structured, meaningful mock data so we are ready to answer our key analytical questions.